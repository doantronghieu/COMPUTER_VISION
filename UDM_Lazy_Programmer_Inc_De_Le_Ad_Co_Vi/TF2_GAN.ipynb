{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Map inputs to (-1, +1) for better training\n",
    "x_train = x_train / 255.0 * 2 - 1\n",
    "x_test  = x_test  / 255.0 * 2 - 1\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the data\n",
    "N, H, W = x_train.shape\n",
    "D       = H * W\n",
    "x_train = x_train.reshape(-1, D)\n",
    "x_test  = x_test .reshape(-1, D)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the latent space\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the generator model\n",
    "def build_generator(latent_dim):\n",
    "    i = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(units=256,  activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Dense(units=512,  activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Dense(units=1024, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Dense(D, activation='tanh')(x) # Image between -1 & +1\n",
    "    \n",
    "    model = Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the discriminator model\n",
    "def build_discriminator(img_size):\n",
    "    i = Input(shape=(img_size,))\n",
    "    x = Dense(units=512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = Dense(units=256, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = Dense(units=1,   activation='sigmoid')           (x)\n",
    "    \n",
    "    model = Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile both models in preparation for training\n",
    "\n",
    "# Build & compile the D\n",
    "discriminator = build_discriminator(img_size=D)\n",
    "discriminator.compile( loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build & compile the combined model\n",
    "generator = build_generator(latent_dim=latent_dim)\n",
    "\n",
    "# Create an input to represent noise sample from latent space\n",
    "z = Input(shape=(latent_dim,))\n",
    "# Pass noise through generator to get an image\n",
    "img = generator(z)\n",
    "\n",
    "# Make sure only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The true output is fake, but we label them real!\n",
    "fake_pred = discriminator(img)\n",
    "\n",
    "# Create the combined model object\n",
    "combined_model = Model(z, fake_pred)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAN\n",
    "\n",
    "# Config\n",
    "batch_size    = 32\n",
    "epochs        = 30000\n",
    "sample_period = 200 # Every `sample_period` steps generate and save some data\n",
    "\n",
    "# Create batch labels to use when calling train_on_batch\n",
    "ones  = np.ones (batch_size)\n",
    "zeros = np.zeros(batch_size)\n",
    "\n",
    "# Store the losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Create a folder to store generated images\n",
    "if not os.path.exists('gan_images'):\n",
    "    os.makedirs('gan_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to generate a grid of random samples from the generator and save them to a file\n",
    "def sample_images(epoch):\n",
    "    rows, cols = 5, 5\n",
    "    noise      = np.random.randn(rows * cols, latent_dim) # Generator random noise vector from the latent space\n",
    "    imgs       = generator.predict(noise)                 # Get generated samples (-1 -> +1)\n",
    "    \n",
    "    # Rescale images 0 - 1\n",
    "    imgs = 0.5 * imgs + 0.5\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "    idx      = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            axs[i, j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            idx += 1\n",
    "        fig.savefig(f'gan_images/{epoch}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    #######################\n",
    "    # Train discriminator #\n",
    "    #######################\n",
    "    \n",
    "    # Select a random batch of images\n",
    "    idx       = np.random.randint(low=0, high=x_train.shape[0], size=batch_size)\n",
    "    real_imgs = x_train[idx]\n",
    "    \n",
    "    # Generate fake images\n",
    "    noise     = np.random.randn(batch_size, latent_dim)\n",
    "    fake_imgs = generator.predict(noise)\n",
    "    \n",
    "    # Both loss & accuracy are returned\n",
    "    d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
    "    d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
    "    d_loss                  = (d_loss_real + d_loss_fake) / 2\n",
    "    d_acc                   = (d_acc_real  + d_acc_fake)  / 2\n",
    "    \n",
    "    ###################\n",
    "    # Train generator #\n",
    "    ###################\n",
    "    \n",
    "    noise  = np.random.randn(batch_size, latent_dim)\n",
    "    g_loss = combined_model.train_on_batch(noise, ones)\n",
    "    \n",
    "    # Save the losses\n",
    "    d_losses.append(d_loss)\n",
    "    g_losses.append(g_loss)\n",
    "    \n",
    "    if (epoch % 100 == 0):\n",
    "        print(f'Epoch: {epoch+1:<5}/{epochs}, d_loss: {d_loss:<4.2f}, d_acc: {d_acc:<4.2f}, g_loss: {g_loss:<4.2f}')\n",
    "    if (epoch % sample_period == 0):\n",
    "        sample_images(epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
